{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d495a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import numpy as nnp\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3203f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.layers import convolution_pooling_op, fully_connected_op, ProbExtractionLayer, PatchedTorchLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bab1d501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14fc53130>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c74b30",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "334f9ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(num_images: int, size: int = 4, noise: float = 0.2):\n",
    "    \"\"\"Generate a vertical horizontal left diagonal or right diagonal line on the grid and then add noise in to it\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for _ in range(num_images):\n",
    "        # Create a blank image\n",
    "        image = np.zeros((size, size), dtype=np.uint16)\n",
    "        # Randomly choose a line orientation\n",
    "        if np.random.rand() < 0.25:\n",
    "            # Vertical line\n",
    "            x = np.random.randint(0, size)\n",
    "            image[:, x] = 255\n",
    "            labels.append(0)  # Label for vertical line\n",
    "        elif np.random.rand() < 0.5:\n",
    "            # Horizontal line\n",
    "            y = np.random.randint(0, size)\n",
    "            image[y, :] = 255\n",
    "            labels.append(1)\n",
    "        elif np.random.rand() < 0.75:\n",
    "            # Left diagonal line\n",
    "            for j in range(size):\n",
    "                image[j, j] = 255\n",
    "            labels.append(2)\n",
    "        else:\n",
    "            # Right diagonal line\n",
    "            for j in range(size):\n",
    "                image[j, size - j - 1] = 255\n",
    "            labels.append(3)\n",
    "\n",
    "        # Add noise to the image\n",
    "        noise_matrix = np.random.normal(0, noise * 255, (size, size))\n",
    "        image = np.clip(image + noise_matrix, 0.0, 255.0)\n",
    "        images.append(image.astype(np.float32) / 255.0)\n",
    "\n",
    "    # one hot encode the labels\n",
    "    labels = np.array(labels)\n",
    "    labels = np.eye(4)[labels]\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43399685",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = generate_dataset(100, noise=0.1)\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    images, labels, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5f9a199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(train_images[i].reshape(train_images.shape[1], train_images.shape[2]), cmap='gray')\n",
    "    plt.title(f\"Label: {train_labels[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e6e2eb",
   "metadata": {},
   "source": [
    "# Creating Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7ac6339",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_images.shape[1] == train_images.shape[2], \"Images must be square\"\n",
    "N = train_images.shape[1]\n",
    "B = 4\n",
    "dev = qml.device(\"default.qubit\", wires=N * N + B)\n",
    "wire_arr = nnp.arange(N * N).reshape(N, N)\n",
    "\n",
    "KERNEL_SIZE = 2\n",
    "KERNEL_LAYERS = 2\n",
    "STRIDE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c24c6c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def qnode(inputs, \n",
    "          first_kernel, first_pooling, \n",
    "        #   second_kernel, second_pooling, \n",
    "          fc_weights, fc_bias):\n",
    "    # Input Layer\n",
    "    for i, j in itertools.product(range(N), range(N)):\n",
    "        qml.RX(2 * np.pi * inputs[i, j], wires=wire_arr[i, j])\n",
    "    \n",
    "    # First Convolution Layer    \n",
    "    convolution_pooling_op(first_kernel, first_pooling, wire_arr, STRIDE)\n",
    "    reduced_wire_arr = wire_arr[1::2, 1::2]\n",
    "    \n",
    "    # Second Convolution Layer\n",
    "    # convolution_pooling_op(second_kernel, second_pooling, reduced_wire_arr, STRIDE)\n",
    "    # reduced_wire_arr = reduced_wire_arr[1::2, 1::2]\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    fully_connected_op(fc_weights, fc_bias, reduced_wire_arr.flatten().tolist(), list(range(N*N, N*N + B)))\n",
    "    \n",
    "    # Measurement\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in reduced_wire_arr.flatten().tolist()]\n",
    "    # return [qml.probs(i) for i in reduced_wire_arr.flatten().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "876abf89",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Quantum Torch Layer: func=qnode>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_shapes = {\n",
    "    \"first_kernel\": (KERNEL_LAYERS, KERNEL_SIZE, KERNEL_SIZE),\n",
    "    \"first_pooling\": (KERNEL_SIZE, KERNEL_SIZE),\n",
    "    # \"second_kernel\": (KERNEL_LAYERS, KERNEL_SIZE, KERNEL_SIZE),\n",
    "    # \"second_pooling\": (KERNEL_SIZE, KERNEL_SIZE),\n",
    "    \"fc_weights\": (B - 1, B),\n",
    "    \"fc_bias\": (B,),\n",
    "}\n",
    "\n",
    "qlayer = PatchedTorchLayer(qnode, weight_shapes)\n",
    "qlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60176fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('qlayer.first_kernel', torch.Size([2, 2, 2])),\n",
       " ('qlayer.first_pooling', torch.Size([2, 2])),\n",
       " ('qlayer.fc_weights', torch.Size([3, 4])),\n",
       " ('qlayer.fc_bias', torch.Size([4]))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    OrderedDict(\n",
    "        [\n",
    "            (\"qlayer\", qlayer),\n",
    "            # (\"extractor\", ProbExtractionLayer()),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "[(k,v.shape) for k,v in model.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e849def5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (qlayer): <Quantum Torch Layer: func=qnode>\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6feb099e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0019, -0.0024,  0.0107, -0.0011], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.tensor(train_images[:1], requires_grad=True)\n",
    "model(img[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4e01b",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d1a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(model.parameters(), lr=0.2)\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951bf3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(train_images, requires_grad=True).float()\n",
    "y_hot = torch.tensor(train_labels, requires_grad=True).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0a8eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[20, -1]' is invalid for input of size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xs, ys \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     13\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m     loss_evaluated \u001b[38;5;241m=\u001b[39m loss(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m, ys)\n\u001b[1;32m     16\u001b[0m     loss_evaluated\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/pennylane/qnn/torch.py:404\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    401\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(inputs, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# calculate the forward pass as usual\u001b[39;00m\n\u001b[0;32m--> 404\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_qnode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_batch_dim:\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/pennylane/qnn/torch.py:445\u001b[0m, in \u001b[0;36mTorchLayer._evaluate_qnode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(_combine_dimensions([r]) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m res)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(_combine_dimensions(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m res)\n\u001b[0;32m--> 445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_combine_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/pennylane/qnn/torch.py:437\u001b[0m, in \u001b[0;36mTorchLayer._evaluate_qnode.<locals>._combine_dimensions\u001b[0;34m(_res)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_combine_dimensions\u001b[39m(_res):\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 437\u001b[0m         _res \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mreshape(r, (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m _res]\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mhstack(_res)\u001b[38;5;241m.\u001b[39mtype(x\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/pennylane/qnn/torch.py:437\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_combine_dimensions\u001b[39m(_res):\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 437\u001b[0m         _res \u001b[38;5;241m=\u001b[39m [\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m _res]\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mhstack(_res)\u001b[38;5;241m.\u001b[39mtype(x\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[20, -1]' is invalid for input of size 2"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "batches = 100 // batch_size\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    list(zip(X, y_hot)), batch_size=5, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for xs, ys in data_loader:\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        loss_evaluated = loss(model(xs), ys)\n",
    "        loss_evaluated.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        running_loss += loss_evaluated\n",
    "    \n",
    "    avg_loss = running_loss / batches\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss.item():.4f}\")\n",
    "\n",
    "y_pred = model(X)\n",
    "predictions = torch.argmax(y_pred, dim=1).detach().numpy()\n",
    "correct = np.sum(predictions == np.argmax(train_labels, axis=1))\n",
    "accuracy = correct / len(train_labels)\n",
    "print(f\"Training accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

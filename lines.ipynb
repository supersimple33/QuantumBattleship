{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d495a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import importlib\n",
    "from functools import reduce, partial\n",
    "import operator\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import numpy as nnp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import torch\n",
    "# torch.set_default_dtype(torch.float32)\n",
    "# mps_device = torch.device(\"mps\")\n",
    "# torch.set_default_device(mps_device)\n",
    "# from torchinfo import summary\n",
    "\n",
    "import tensorflow as tf\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1af02b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# import traceback\n",
    "\n",
    "# warnings.simplefilter(\"error\")\n",
    "# warnings.simplefilter(\"once\", category=qml.PennyLaneDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd3203f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab1d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "nnp.random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "# torch.manual_seed(42)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c74b30",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334f9ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_4(num_images: int, size: int = 4, noise: float = 0.15):\n",
    "    \"\"\"Generate a vertical horizontal left diagonal or right diagonal line on the grid and then add noise in to it\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for _ in range(num_images):\n",
    "        # Create a blank image\n",
    "        image = np.zeros((size, size), dtype=np.uint16)\n",
    "        # Randomly choose a line orientation\n",
    "        if np.random.rand() < 0.25:\n",
    "            # Vertical line\n",
    "            x = np.random.randint(0, size)\n",
    "            image[:, x] = 255\n",
    "            labels.append(0)  # Label for vertical line\n",
    "        elif np.random.rand() < 0.5:\n",
    "            # Horizontal line\n",
    "            y = np.random.randint(0, size)\n",
    "            image[y, :] = 255\n",
    "            labels.append(1)\n",
    "        elif np.random.rand() < 0.75:\n",
    "            # Left diagonal line\n",
    "            for j in range(size):\n",
    "                image[j, j] = 255\n",
    "            labels.append(2)\n",
    "        else:\n",
    "            # Right diagonal line\n",
    "            for j in range(size):\n",
    "                image[j, size - j - 1] = 255\n",
    "            labels.append(3)\n",
    "\n",
    "        # Add noise to the image\n",
    "        noise_matrix = np.random.normal(0, noise * 255, (size, size))\n",
    "        image = np.clip(image + noise_matrix, 0.0, 255.0)\n",
    "        images.append(image.astype(np.float32) / 255.0)\n",
    "\n",
    "    # one hot encode the labels\n",
    "    labels = np.array(labels)\n",
    "    labels = ((-1 * np.ones((4,4))) + (2 * np.eye(4)))[labels]\n",
    "    return nnp.array(images), nnp.array(labels).astype(nnp.float32)\n",
    "\n",
    "def generate_dataset_2(num_images: int, size: int = 4, noise: float = 0.15):\n",
    "    \"\"\"Generate a vertical or horizontal line on the grid and then add noise in to it\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for _ in range(num_images):\n",
    "        # Create a blank image\n",
    "        image = np.zeros((size, size), dtype=np.uint16)\n",
    "        # Randomly choose a line orientation\n",
    "        if np.random.rand() < 0.5:\n",
    "            # Vertical line\n",
    "            x = np.random.randint(0, size)\n",
    "            image[:, x] = 255\n",
    "            labels.append(-1.0)  # Label for vertical line\n",
    "        else:\n",
    "            # Horizontal line\n",
    "            y = np.random.randint(0, size)\n",
    "            image[y, :] = 255\n",
    "            labels.append(1.0)\n",
    "\n",
    "        # Add noise to the image\n",
    "        noise_matrix = np.random.normal(0, noise * 255, (size, size))\n",
    "        image = np.clip(image + noise_matrix, 0.0, 255.0)\n",
    "        images.append(image.astype(np.float32) / 255.0)\n",
    "\n",
    "    # one hot encode the labels\n",
    "    # labels = np.array(labels)\n",
    "    # labels = np.eye(2)[labels]\n",
    "    return nnp.array(images), nnp.array(labels).astype(nnp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43399685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = generate_dataset_2(200, noise=0.00)\n",
    "\n",
    "# Split the data\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    images, labels, test_size=0.3, random_state=42\n",
    ")\n",
    "# NOTE: Pennylane will freak out if the number of training images is not divisible by the batch size\n",
    "test_labels.dtype, test_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5f9a199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(train_images[i].reshape(int(train_images.shape[1]), int(train_images.shape[2])), cmap='gray')\n",
    "    plt.title(f\"Label: {train_labels[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "067ce93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images = nnp.array([hots_to_sv(img.flatten()) for img in train_images]), nnp.array([hots_to_sv(img.flatten()) for img in test_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "010162bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 15:05:55.381739: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2025-05-30 15:05:55.381772: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2025-05-30 15:05:55.381780: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748631955.382302 12680891 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1748631955.382543 12680891 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tf.complex64,\n",
       " tf.float32,\n",
       " tf.complex64,\n",
       " tf.float32,\n",
       " '/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " TensorShape([140, 65536]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images, train_labels = tf.convert_to_tensor(train_images), tf.convert_to_tensor(train_labels)\n",
    "test_images, test_labels = tf.convert_to_tensor(test_images), tf.convert_to_tensor(test_labels)\n",
    "train_images.dtype, train_labels.dtype, test_images.dtype, test_labels.dtype, train_images.device, train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e6e2eb",
   "metadata": {},
   "source": [
    "# Creating Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7ac6339",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_images.shape[1] ** 0.5 % 1 == 0, \"The input image size must be a perfect square\"\n",
    "B = 4\n",
    "N = 4\n",
    "w = N**2 # + B\n",
    "dev = qml.device(\"default.qubit\", wires=w)\n",
    "wire_arr = nnp.arange(N**2, dtype=nnp.int32).reshape(N, N)\n",
    "\n",
    "KERNEL_SIZE = 2\n",
    "KERNEL_LAYERS = 2 # two was working pretty well\n",
    "STRIDE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c24c6c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @partial(qml.batch_input, argnums=0) # this is really broken (need to file a pennylane issue)\n",
    "@partial(qml.batch_input, argnum=0)\n",
    "@qml.qnode(dev, interface='tf')\n",
    "def qnode(inputs, \n",
    "          first_kernel, first_pooling, \n",
    "          second_kernel, second_pooling, \n",
    "          # fc_weights, fc_bias\n",
    "):\n",
    "    # Input Layer\n",
    "    # for i, j in itertools.product(range(N), range(N)):\n",
    "    #     qml.RX(1.0 * np.pi * inputs[i, j], wires=wire_arr[i, j])\n",
    "    qml.StatePrep(inputs, wires=wire_arr.flatten(), validate_norm=False)\n",
    "    \n",
    "    # First Convolution Layer    \n",
    "    convolution_pooling_op(first_kernel, first_pooling, wire_arr, STRIDE)\n",
    "    reduced_wire_arr = wire_arr[1::2, 1::2]\n",
    "    \n",
    "    # Second Convolution Layer\n",
    "    convolution_pooling_op(second_kernel, second_pooling, reduced_wire_arr, STRIDE)\n",
    "    reduced_wire_arr = reduced_wire_arr[1::2, 1::2]\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    # fully_connected_op(fc_weights, fc_bias, reduced_wire_arr.flatten().tolist(), list(range(N*N, N*N + B)))\n",
    "    \n",
    "    # Measurement\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in reduced_wire_arr.flatten().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "876abf89",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/addisonhanrattie/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/pennylane/qnn/keras.py:317: PennyLaneDeprecationWarning: The 'KerasLayer' class is deprecated and will be removed in v0.42. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'first_kernel': (8, TensorShape([2, 2, 2])),\n",
       "  'first_pooling': (4, TensorShape([2, 2])),\n",
       "  'second_kernel': (8, TensorShape([2, 2, 2])),\n",
       "  'second_pooling': (4, TensorShape([2, 2]))},\n",
       " <Quantum Keras Layer: func=qnode>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_shapes = {\n",
    "    \"first_kernel\": (KERNEL_LAYERS, KERNEL_SIZE, KERNEL_SIZE),\n",
    "    \"first_pooling\": (KERNEL_SIZE, KERNEL_SIZE),\n",
    "    \"second_kernel\": (KERNEL_LAYERS, KERNEL_SIZE, KERNEL_SIZE),\n",
    "    \"second_pooling\": (KERNEL_SIZE, KERNEL_SIZE),\n",
    "    # \"fc_weights\": (B - 1, B),\n",
    "    # \"fc_bias\": (B,),\n",
    "}\n",
    "\n",
    "qlayer = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=(1,))\n",
    "{name: (reduce(operator.mul, x.shape), x.shape) for name, x in qlayer.qnode_weights.items()}, qlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60176fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(2**(N * N),)),\n",
    "    qlayer,\n",
    "])\n",
    "# model.load_weights('line_model.keras')\n",
    "# model = torch.nn.Sequential(\n",
    "#     qlayer,\n",
    "#     # torch.nn.Lambda(prob_extraction),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f558566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ keras_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">KerasLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ keras_layer (\u001b[38;5;33mKerasLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m24\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> (96.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24\u001b[0m (96.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> (96.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m24\u001b[0m (96.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "model.compile(opt, loss=\"MSE\", metrics=[custom_accuracy])\n",
    "model.summary()\n",
    "# summary(model, input_data=train_images[0:3, :], device=mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b06efe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n",
      "Sample output shape: (7, 1)\n",
      "Sample output: [[ 0.01225899]\n",
      " [-0.00081922]\n",
      " [-0.01116229]\n",
      " [-0.00227868]\n",
      " [-0.00227868]\n",
      " [-0.00227868]\n",
      " [-0.01354312]] tf.Tensor([-1.  1. -1.  1.  1.  1. -1.], shape=(7,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test the forward pass with a batch of training images\n",
    "sample_output = model(train_images[21:21+7])  # Pass the first 3 training images\n",
    "print(\"Sample output shape:\", sample_output.shape)\n",
    "print(\"Sample output:\", sample_output.numpy(), train_labels[21:21+7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4e01b",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd44fafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([128, 65536]),\n",
       " TensorShape([128]),\n",
       " TensorShape([48, 65536]),\n",
       " TensorShape([48]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "remainder = train_images.shape[0] % BATCH_SIZE\n",
    "if remainder != 0:\n",
    "    train_images = train_images[:-remainder]\n",
    "    train_labels = train_labels[:-remainder]\n",
    "remainder2 = test_images.shape[0] % BATCH_SIZE\n",
    "if remainder2 != 0:\n",
    "    test_images = test_images[:-remainder2]\n",
    "    test_labels = test_labels[:-remainder2]\n",
    "train_images.shape, train_labels.shape, test_images.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7c4a426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 24s/step - custom_accuracy: 0.6609 - loss: 0.9920 - val_custom_accuracy: 0.9167 - val_loss: 0.9734\n",
      "Epoch 2/4\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 16s/step - custom_accuracy: 0.9232 - loss: 0.9763 - val_custom_accuracy: 1.0000 - val_loss: 0.9620\n",
      "Epoch 3/4\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 16s/step - custom_accuracy: 0.9242 - loss: 0.9613 - val_custom_accuracy: 0.7500 - val_loss: 0.9487\n",
      "Epoch 4/4\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 14s/step - custom_accuracy: 0.7160 - loss: 0.9196 - val_custom_accuracy: 0.7500 - val_loss: 0.8673\n"
     ]
    }
   ],
   "source": [
    "silence_tensorflow(\"ERROR\")\n",
    "fitting = model.fit(train_images, train_labels, epochs=4, batch_size=BATCH_SIZE, validation_data=(test_images, test_labels), verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf84bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1574328",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/line_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3504614",
   "metadata": {},
   "source": [
    "# A Classical Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797c8b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edea98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(N, N, 1)),\n",
    "    tf.keras.layers.Conv2D(2, kernel_size=KERNEL_SIZE, strides=STRIDE, use_bias=True, padding='SAME'),\n",
    "    tf.keras.layers.SeparableConv2D(1, kernel_size=KERNEL_SIZE, strides=STRIDE, use_bias=True, padding='SAME'),\n",
    "    tf.keras.layers.AveragePooling2D(pool_size=KERNEL_SIZE),\n",
    "    tf.keras.layers.Conv2D(2, kernel_size=KERNEL_SIZE, strides=STRIDE, use_bias=True, padding='SAME'),\n",
    "    tf.keras.layers.SeparableConv2D(1, kernel_size=KERNEL_SIZE, strides=STRIDE, use_bias=True, padding='SAME'),\n",
    "    tf.keras.layers.AveragePooling2D(pool_size=KERNEL_SIZE),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    # tf.keras.layers.Dense(8, use_bias=True, activation='relu'),\n",
    "    # tf.keras.layers.Dense(8, use_bias=True, activation='relu'),\n",
    "    # tf.keras.layers.Dense(8, use_bias=True, activation='relu'),\n",
    "    # tf.keras.layers.Dense(1, use_bias=True, activation='sigmoid'),\n",
    "    \n",
    "    \n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "])\n",
    "classic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81af4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_labels = tf.where(train_labels > 0.5, 1.0, 0.0)\n",
    "classic_test_labels = tf.where(test_labels > 0.5, 1.0, 0.0)\n",
    "# classic_model(train_images[..., np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce5b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_opt = tf.keras.optimizers.Adagrad(learning_rate=0.025)\n",
    "classic_model.compile(classic_opt, loss=\"CrossEntropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba8399",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting = classic_model.fit(train_images, classic_labels, epochs=100, batch_size=16, validation_data=(test_images, classic_test_labels), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf51f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = classic_model.evaluate(test_images, classic_test_labels, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e02f0",
   "metadata": {},
   "source": [
    "# A Better Classical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(N, N)),\n",
    "    # tf.keras.layers.Conv2D(4, kernel_size=4, strides=STRIDE, use_bias=True, padding='SAME'),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(8, use_bias=True, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, use_bias=True, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, use_bias=True, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, use_bias=True, activation='sigmoid'),\n",
    "])\n",
    "classic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca3afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_labels = tf.where(train_labels > 0.5, 1.0, 0.0)\n",
    "classic_test_labels = tf.where(test_labels > 0.5, 1.0, 0.0)\n",
    "# classic_model(train_images[..., np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f797b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_opt = tf.keras.optimizers.Adagrad(learning_rate=0.025)\n",
    "classic_model.compile(classic_opt, loss=\"CrossEntropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcffbef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting = classic_model.fit(train_images, classic_labels, epochs=100, batch_size=4, validation_data=(test_images, classic_test_labels), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf2f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = classic_model.evaluate(test_images, classic_test_labels, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

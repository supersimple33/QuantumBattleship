{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d495a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import importlib\n",
    "from functools import reduce, partial\n",
    "import operator\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import numpy as nnp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import torch\n",
    "# torch.set_default_dtype(torch.float32)\n",
    "# mps_device = torch.device(\"mps\")\n",
    "# torch.set_default_device(mps_device)\n",
    "# from torchinfo import summary\n",
    "\n",
    "import tensorflow as tf\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1af02b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# import traceback\n",
    "\n",
    "# warnings.simplefilter(\"error\")\n",
    "# warnings.simplefilter(\"once\", category=qml.PennyLaneDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd3203f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab1d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "nnp.random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "# torch.manual_seed(42)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c74b30",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334f9ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_4(num_images: int, size: int = 4, noise: float = 0.15):\n",
    "    \"\"\"Generate a vertical horizontal left diagonal or right diagonal line on the grid and then add noise in to it\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for _ in range(num_images):\n",
    "        # Create a blank image\n",
    "        image = np.zeros((size, size), dtype=np.uint16)\n",
    "        # Randomly choose a line orientation\n",
    "        if np.random.rand() < 0.25:\n",
    "            # Vertical line\n",
    "            x = np.random.randint(0, size)\n",
    "            image[:, x] = 255\n",
    "            labels.append(0)  # Label for vertical line\n",
    "        elif np.random.rand() < 0.5:\n",
    "            # Horizontal line\n",
    "            y = np.random.randint(0, size)\n",
    "            image[y, :] = 255\n",
    "            labels.append(1)\n",
    "        elif np.random.rand() < 0.75:\n",
    "            # Left diagonal line\n",
    "            for j in range(size):\n",
    "                image[j, j] = 255\n",
    "            labels.append(2)\n",
    "        else:\n",
    "            # Right diagonal line\n",
    "            for j in range(size):\n",
    "                image[j, size - j - 1] = 255\n",
    "            labels.append(3)\n",
    "\n",
    "        # Add noise to the image\n",
    "        noise_matrix = np.random.normal(0, noise * 255, (size, size))\n",
    "        image = np.clip(image + noise_matrix, 0.0, 255.0)\n",
    "        images.append(image.astype(np.float32) / 255.0)\n",
    "\n",
    "    # one hot encode the labels\n",
    "    labels = np.array(labels)\n",
    "    labels = ((-1 * np.ones((4,4))) + (2 * np.eye(4)))[labels]\n",
    "    return nnp.array(images), nnp.array(labels).astype(nnp.float32)\n",
    "\n",
    "def generate_dataset_2(num_images: int, size: int = 4, noise: float = 0.15):\n",
    "    \"\"\"Generate a vertical or horizontal line on the grid and then add noise in to it\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for _ in range(num_images):\n",
    "        # Create a blank image\n",
    "        image = np.zeros((size, size), dtype=np.uint16)\n",
    "        # Randomly choose a line orientation\n",
    "        if np.random.rand() < 0.5:\n",
    "            # Vertical line\n",
    "            x = np.random.randint(0, size)\n",
    "            image[:, x] = 255\n",
    "            labels.append(-1.0)  # Label for vertical line\n",
    "        else:\n",
    "            # Horizontal line\n",
    "            y = np.random.randint(0, size)\n",
    "            image[y, :] = 255\n",
    "            labels.append(1.0)\n",
    "\n",
    "        # Add noise to the image\n",
    "        noise_matrix = np.random.normal(0, noise * 255, (size, size))\n",
    "        image = np.clip(image + noise_matrix, 0.0, 255.0)\n",
    "        images.append(image.astype(np.float32) / 255.0)\n",
    "\n",
    "    # one hot encode the labels\n",
    "    # labels = np.array(labels)\n",
    "    # labels = np.eye(2)[labels]\n",
    "    return nnp.array(images), nnp.array(labels).astype(nnp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43399685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = generate_dataset_2(150, noise=0.05)\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    images, labels, test_size=0.3, random_state=42\n",
    ")\n",
    "test_labels.dtype, test_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5f9a199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(train_images[i].reshape(int(train_images.shape[1]), int(train_images.shape[2])), cmap='gray')\n",
    "    plt.title(f\"Label: {train_labels[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "067ce93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, test_images = nnp.array([hots_to_sv(img.flatten()) for img in train_images]), nnp.array([hots_to_sv(img.flatten()) for img in test_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "010162bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 10:41:14.332865: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2025-05-30 10:41:14.332898: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2025-05-30 10:41:14.332905: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748616074.332919 12193522 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1748616074.332938 12193522 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tf.complex64,\n",
       " '/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " TensorShape([105, 65536]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images, train_labels = tf.convert_to_tensor(train_images), tf.convert_to_tensor(train_labels)\n",
    "test_images, test_labels = tf.convert_to_tensor(test_images), tf.convert_to_tensor(test_labels)\n",
    "train_images.dtype, train_images.device, train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e6e2eb",
   "metadata": {},
   "source": [
    "# Creating Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7ac6339",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_images.shape[1] ** 0.5 % 1 == 0, \"The input image size must be a perfect square\"\n",
    "B = 4\n",
    "N = 4\n",
    "w = N**2 # + B\n",
    "dev = qml.device(\"default.qubit\", wires=w)\n",
    "wire_arr = nnp.arange(N**2, dtype=nnp.int32).reshape(N, N)\n",
    "\n",
    "KERNEL_SIZE = 2\n",
    "KERNEL_LAYERS = 2 # two was working pretty well\n",
    "STRIDE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c24c6c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @partial(qml.batch_input, argnums=0) # this is really broken (need to file a pennylane issue)\n",
    "@partial(qml.batch_input, argnum=0)\n",
    "@qml.qnode(dev, interface='tf')\n",
    "def qnode(inputs, \n",
    "          first_kernel, first_pooling, \n",
    "          second_kernel, second_pooling, \n",
    "          # fc_weights, fc_bias\n",
    "):\n",
    "    # Input Layer\n",
    "    # for i, j in itertools.product(range(N), range(N)):\n",
    "    #     qml.RX(1.0 * np.pi * inputs[i, j], wires=wire_arr[i, j])\n",
    "    qml.StatePrep(inputs, wires=wire_arr.flatten(), validate_norm=False)\n",
    "    \n",
    "    # First Convolution Layer    \n",
    "    convolution_pooling_op(first_kernel, first_pooling, wire_arr, STRIDE)\n",
    "    reduced_wire_arr = wire_arr[1::2, 1::2]\n",
    "    \n",
    "    # Second Convolution Layer\n",
    "    convolution_pooling_op(second_kernel, second_pooling, reduced_wire_arr, STRIDE)\n",
    "    reduced_wire_arr = reduced_wire_arr[1::2, 1::2]\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    # fully_connected_op(fc_weights, fc_bias, reduced_wire_arr.flatten().tolist(), list(range(N*N, N*N + B)))\n",
    "    \n",
    "    # Measurement\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in reduced_wire_arr.flatten().tolist()]\n",
    "    # return [qml.probs(i) for i in reduced_wire_arr.flatten().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "876abf89",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/addisonhanrattie/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/pennylane/qnn/keras.py:317: PennyLaneDeprecationWarning: The 'KerasLayer' class is deprecated and will be removed in v0.42. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'first_kernel': (8, TensorShape([2, 2, 2])),\n",
       "  'first_pooling': (4, TensorShape([2, 2])),\n",
       "  'second_kernel': (8, TensorShape([2, 2, 2])),\n",
       "  'second_pooling': (4, TensorShape([2, 2]))},\n",
       " <Quantum Keras Layer: func=qnode>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_shapes = {\n",
    "    \"first_kernel\": (KERNEL_LAYERS, KERNEL_SIZE, KERNEL_SIZE),\n",
    "    \"first_pooling\": (KERNEL_SIZE, KERNEL_SIZE),\n",
    "    \"second_kernel\": (KERNEL_LAYERS, KERNEL_SIZE, KERNEL_SIZE),\n",
    "    \"second_pooling\": (KERNEL_SIZE, KERNEL_SIZE),\n",
    "    # \"fc_weights\": (B - 1, B),\n",
    "    # \"fc_bias\": (B,),\n",
    "}\n",
    "\n",
    "qlayer = qml.qnn.KerasLayer(qnode, weight_shapes, output_dim=(1,))\n",
    "{name: (reduce(operator.mul, x.shape), x.shape) for name, x in qlayer.qnode_weights.items()}, qlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60176fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(2**(N * N),)),\n",
    "    qlayer,\n",
    "])\n",
    "# model.load_weights('line_model.keras')\n",
    "# model = torch.nn.Sequential(\n",
    "#     qlayer,\n",
    "#     # torch.nn.Lambda(prob_extraction),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f558566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ keras_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">KerasLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ keras_layer (\u001b[38;5;33mKerasLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m24\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> (96.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24\u001b[0m (96.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> (96.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m24\u001b[0m (96.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.025)\n",
    "model.compile(opt, loss=\"MSE\", metrics=[custom_accuracy])\n",
    "model.summary()\n",
    "# summary(model, input_data=train_images[0:3, :], device=mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b06efe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:You are casting an input of type complex64 to an incompatible dtype float32.  This will discard the imaginary part and may not be what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample output shape: (3, 1)\n",
      "Sample output: [[ 0.01905141]\n",
      " [-0.00619879]\n",
      " [ 0.01904069]] tf.Tensor([ 1. -1.  1.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test the forward pass with a batch of training images\n",
    "sample_output = model(train_images[:3])  # Pass the first 3 training images\n",
    "print(\"Sample output shape:\", sample_output.shape)\n",
    "print(\"Sample output:\", sample_output.numpy(), train_labels[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4e01b",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c4a426",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling KerasLayer.call().\n\n\u001b[1m'NoneType' object cannot be interpreted as an integer\u001b[0m\n\nArguments received by KerasLayer.call():\n  • inputs=tf.Tensor(shape=(None, 65536), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m silence_tensorflow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m fitting \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/pennylane/qnn/keras.py:421\u001b[0m, in \u001b[0;36mKerasLayer.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    418\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(inputs, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# calculate the forward pass as usual\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_qnode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# reshape to the correct number of batch dims\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_batch_dim:\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# pylint:disable=unexpected-keyword-arg,no-value-for-parameter\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/pennylane/qnn/keras.py:444\u001b[0m, in \u001b[0;36mKerasLayer._evaluate_qnode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluates a QNode for a single input datapoint.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m    tensor: output datapoint\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    440\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_arg: x},\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m*\u001b[39m w \u001b[38;5;28;01mfor\u001b[39;00m k, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnode_weights\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[1;32m    443\u001b[0m }\n\u001b[0;32m--> 444\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;66;03m# multi-return and batch dim case\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/pennylane/workflow/qnode.py:882\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_capture_qnode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m capture_qnode  \u001b[38;5;66;03m# pylint: disable=import-outside-toplevel\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m capture_qnode(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 882\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impl_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/pennylane/workflow/qnode.py:855\u001b[0m, in \u001b[0;36mQNode._impl_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;66;03m# Calculate the classical jacobians if necessary\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_program\u001b[38;5;241m.\u001b[39mset_classical_component(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[0;32m--> 855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mqml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiff_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiff_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_program\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    866\u001b[0m \u001b[38;5;66;03m# convert result to the interface in case the qfunc has no parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/pennylane/workflow/execution.py:239\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(tapes, device, diff_method, interface, transform_program, grad_on_execution, cache, cachesize, max_diff, device_vjp, postselect_mode, mcm_method, gradient_kwargs, mcm_config, config, inner_transform)\u001b[0m\n\u001b[1;32m    234\u001b[0m transform_program, inner_transform \u001b[38;5;241m=\u001b[39m _setup_transform_program(\n\u001b[1;32m    235\u001b[0m     transform_program, device, config, cache, cachesize\n\u001b[1;32m    236\u001b[0m )\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m#### Executing the configured setup #####\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m tapes, post_processing \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_program\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transform_program\u001b[38;5;241m.\u001b[39mis_informative:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m post_processing(tapes)\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/pennylane/transforms/core/transform_program.py:643\u001b[0m, in \u001b[0;36mTransformProgram.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(args[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJaxpr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__call_jaxpr(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__call_tapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/pennylane/transforms/core/transform_program.py:580\u001b[0m, in \u001b[0;36mTransformProgram.__call_tapes\u001b[0;34m(self, tapes)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argnums \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    579\u001b[0m     tape\u001b[38;5;241m.\u001b[39mtrainable_params \u001b[38;5;241m=\u001b[39m argnums[j]\n\u001b[0;32m--> 580\u001b[0m new_tapes, fn \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m execution_tapes\u001b[38;5;241m.\u001b[39mextend(new_tapes)\n\u001b[1;32m    583\u001b[0m fns\u001b[38;5;241m.\u001b[39mappend(fn)\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/pennylane/transforms/batch_input.py:106\u001b[0m, in \u001b[0;36mbatch_input\u001b[0;34m(tape, argnum)\u001b[0m\n\u001b[1;32m    103\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m batch_dims[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    105\u001b[0m output_tapes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ops \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_split_operations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    107\u001b[0m     new_tape \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mtape\u001b[38;5;241m.\u001b[39mQuantumScript(\n\u001b[1;32m    108\u001b[0m         ops, tape\u001b[38;5;241m.\u001b[39mmeasurements, shots\u001b[38;5;241m=\u001b[39mtape\u001b[38;5;241m.\u001b[39mshots, trainable_params\u001b[38;5;241m=\u001b[39mtape\u001b[38;5;241m.\u001b[39mtrainable_params\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    110\u001b[0m     output_tapes\u001b[38;5;241m.\u001b[39mappend(new_tape)\n",
      "File \u001b[0;32m~/Downloads/QuantumBattleship.nosync/.conda/lib/python3.10/site-packages/pennylane/transforms/batch_params.py:60\u001b[0m, in \u001b[0;36m_split_operations\u001b[0;34m(ops, params, split_indices, num_tapes)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mGiven a list of operators, return a list (with length ``num_tapes``) containing lists\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03mof new operators with the parameters at the given indices unbatched.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    num_tapes (int): the number of new tapes to create, which is also equal to the batch size.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# for some reason pylint thinks \"qml.ops\" is a set\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# pylint: disable=no-member\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m new_ops \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_tapes\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     61\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m ops:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# determine if any parameters of the operator are batched\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling KerasLayer.call().\n\n\u001b[1m'NoneType' object cannot be interpreted as an integer\u001b[0m\n\nArguments received by KerasLayer.call():\n  • inputs=tf.Tensor(shape=(None, 65536), dtype=float32)"
     ]
    }
   ],
   "source": [
    "silence_tensorflow(\"ERROR\")\n",
    "fitting = model.fit(train_images, train_labels, epochs=1, batch_size=3, validation_data=(test_images, test_labels), verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf84bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1574328",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/line_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3504614",
   "metadata": {},
   "source": [
    "# A Classical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edea98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(N, N, 1)),\n",
    "    tf.keras.layers.Conv2D(2, kernel_size=KERNEL_SIZE, strides=STRIDE, use_bias=True, padding='SAME'),\n",
    "    tf.keras.layers.SeparableConv2D(1, kernel_size=KERNEL_SIZE, strides=STRIDE, use_bias=True, padding='SAME'),\n",
    "    tf.keras.layers.AveragePooling2D(pool_size=KERNEL_SIZE),\n",
    "    tf.keras.layers.Conv2D(2, kernel_size=KERNEL_SIZE, strides=STRIDE, use_bias=True, padding='SAME'),\n",
    "    tf.keras.layers.SeparableConv2D(1, kernel_size=KERNEL_SIZE, strides=STRIDE, use_bias=True, padding='SAME'),\n",
    "    tf.keras.layers.AveragePooling2D(pool_size=KERNEL_SIZE),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    # tf.keras.layers.Dense(8, use_bias=True, activation='relu'),\n",
    "    # tf.keras.layers.Dense(8, use_bias=True, activation='relu'),\n",
    "    # tf.keras.layers.Dense(8, use_bias=True, activation='relu'),\n",
    "    # tf.keras.layers.Dense(1, use_bias=True, activation='sigmoid'),\n",
    "    \n",
    "    \n",
    "    tf.keras.layers.Activation('sigmoid'),\n",
    "])\n",
    "classic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81af4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_labels = tf.where(train_labels > 0.5, 1.0, 0.0)\n",
    "classic_test_labels = tf.where(test_labels > 0.5, 1.0, 0.0)\n",
    "# classic_model(train_images[..., np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce5b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_opt = tf.keras.optimizers.Adagrad(learning_rate=0.025)\n",
    "classic_model.compile(classic_opt, loss=\"CrossEntropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba8399",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting = classic_model.fit(train_images, classic_labels, epochs=100, batch_size=16, validation_data=(test_images, classic_test_labels), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf51f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = classic_model.evaluate(test_images, classic_test_labels, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e02f0",
   "metadata": {},
   "source": [
    "# A Better Classical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(N, N)),\n",
    "    # tf.keras.layers.Conv2D(4, kernel_size=4, strides=STRIDE, use_bias=True, padding='SAME'),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(8, use_bias=True, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, use_bias=True, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, use_bias=True, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, use_bias=True, activation='sigmoid'),\n",
    "])\n",
    "classic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca3afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_labels = tf.where(train_labels > 0.5, 1.0, 0.0)\n",
    "classic_test_labels = tf.where(test_labels > 0.5, 1.0, 0.0)\n",
    "# classic_model(train_images[..., np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f797b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_opt = tf.keras.optimizers.Adagrad(learning_rate=0.025)\n",
    "classic_model.compile(classic_opt, loss=\"CrossEntropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcffbef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitting = classic_model.fit(train_images, classic_labels, epochs=100, batch_size=4, validation_data=(test_images, classic_test_labels), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cf2f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = classic_model.evaluate(test_images, classic_test_labels, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
